\section{Cluster Parallelisierung}
\textbf{Synchron:} Send \& Recv sind blockierend.\\ 
\textit{mpiexe -n 100 programm.exe}\\
\textbf{Motivation:} möglichst hohe Beschleunigung. Viele CPU Cores statt nur viele GPU Cores.
\textbf{Verteiltes Programmiermodel:} Programm auf mehreren Nodes ausführen. Kein Shared Memory (NUMA) zwischen nodes, nur für Cores im Node (SMP).
\textbf{Message Passing Interface (MPI):} Basiert auf Actor/CSP Prinzip.
\begin{lstlisting}
MPI_Init(&argc, &argv); // MPI Initialisierung 
MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Prozess Identifikation
MPI_Finalize(); // MPI Finalisierung
MPI_Send(&val, 1, MPI_INT, recRank, tag, MPI_COMM_WORLD);
MPI_Recv(&val, 1, MPI_INT, senderRank, tag, MPI_COMM_WORLD, 
                                        MPI_STATUS_IGNORE);
MPI_Barrier(MPI_COMM_WORLD); // Wartet auf alle Prozesse
MPI_Allreduce(&val, &total, 1, MPI_INT, MPI_SUM, 
    MPI_COMM_WORLD); // Aggregation von Teilresultaten
MPI_Reduce(&val, &total, 1, MPI_INT, MPI_SUM, recRank, 
    MPI_COMM_WORLD); // Effizienter, kein Broadcast
\end{lstlisting}
\textbf{SPMD:} Single Program Multiple Data. MPI Programm wird in mehrere Prozesse gestart.
Prozesse können untereinander kommunizieren.
\textbf{Communicator:} Gruppe von MPI-Prozessen. 
Communicator World: Alle Prozesse einer MPI-Programmausführung.
\begin{lstlisting}
// maximum of array
int main(int argc, char* argv[]) {
    MPI_Init(&argc, &argv);
    int rank, size;
    MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &size);
    int* array = NULL; int offset, length = 0;
    if(rank == 0) {
        array = read_array_file(&length);
        int partition = length / size;
        for (int to = 1; to < size; to++) {
            offset = rank * partition;
            if(rank == size - 1) { partition = length - offset; }
            MPI_Send(&partition, 1, MPI_INT, to, 0, MPI_COMM_WORLD);
            MPI_Send(array+offset, partition MPI_INT, to, 0, MPI_COMM_WORLD);
        }
        if(size > 1) { length = partition; }
    } else {
        MPI_Recv(&length, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
        MPI_Recv(array, length, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }
    // calculate maximum 
    int result;
    MPI_Reduce(&maximum, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);
    printf("maximum: %i", result);
    MPI_Finalize();
}
\end{lstlisting}